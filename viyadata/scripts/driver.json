{
	"hadoop": "hadoop fs -ls /",
	"hadoop-put": {
		"pre": "dd if=/dev/urandom of=$WORKDIR/dd.out bs=1K count=1",
		"post": "hadoop fs -rm -f -skipTrash /tmp/eproot-dd.out",
		"cmd": "hadoop fs -put -f $WORKDIR/dd.out /tmp/eproot-dd.out"
	},
	"yarn-node": "yarn node -list",
	"yarn-apps": "yarn application -list",
	"hbase": {
		"pre": "'echo \"whoami\nexit\" > %s/test.hBase' % WORKDIR",
		"cmd": "'hbase shell %s/test.hBase' % WORKDIR"
	},
	"pig": {
		"class": "PigTrace",
		"code": "rmf /tmp/pigtracer.$username/output\nA = LOAD '/tmp/pigtracer.$username/indata' USING PigStorage(',') AS (customer_number,account_number,status);\nB = FILTER A BY status == 'foo';\nstore B into '/tmp/pigtracer.$username/output' USING PigStorage(',');",
		"data": "customer_number,account_number,status\n1,1,foo\n2,2,foo\n3,3,bar\n4,4,foo\n5,5,baz\n6,6,foo"
	},
	"pighcat": {
		"class": "PigTrace",
		"pre": "create table student_order_table (id int, firstname string, lastname string, age int, phone string, city string) row format delimited fields terminated by ',' stored as textfile;",
		"post": "drop table student_order_table;",
		"code": "rmf /tmp/pighcattracer.$username/output\nstudent = LOAD '/tmp/pighcattracer.$username/indata' USING PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray, city:chararray);\nstudent_order = ORDER student BY age DESC;\nSTORE student_order INTO 'student_order_table' USING org.apache.hive.hcatalog.pig.HCatStorer();\nstudent_limit = LIMIT student_order 4;\nDump student_limit;",
		"data": "001,Rajiv,Reddy,21,9848022337,Hyderabad\n002,siddarth,Battacharya,22,9848022338,Kolkata\n003,Rajesh,Khanna,22,9848022339,Delhi\n004,Preethi,Agarwal,21,9848022330,Pune\n005,Trupthi,Mohanthy,23,9848022336,Bhuwaneshwar\n006,Archana,Mishra,23,9848022335,Chennai\n007,Komal,Nayak,24,9848022334,trivendram\n008,Bharathi,Nambiayar,24,9848022333,Chennai"
	},
	"maprlogin": {
		"class": "MaprLoginTrace"
	},
	"checknative": {
		"class": "CheckNative"
	},
	"oozie": {
		"class": "OozieTrace"
	},
        "beeline": {
                "class": "BeelineJdbcTrace"
        },
	"mapreduce": {
		"class": "MapReduceTrace",
		"code": "package org.apache.hadoop.examples;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class WordCount {\n\n  public static class TokenizerMapper\n       extends Mapper<Object, Text, Text, IntWritable>{\n\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString());\n      while (itr.hasMoreTokens()) {\n        word.set(itr.nextToken());\n        context.write(word, one);\n      }\n    }\n  }\n\n  public static class IntSumReducer\n       extends Reducer<Text,IntWritable,Text,IntWritable> {\n    private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable val : values) {\n        sum += val.get();\n      }\n      result.set(sum);\n      context.write(key, result);\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n    if (otherArgs.length < 2) {\n      System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n      System.exit(2);\n    }\n    Job job = new Job(conf, \"word count\");\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(TokenizerMapper.class);\n    job.setCombinerClass(IntSumReducer.class);\n    job.setReducerClass(IntSumReducer.class);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n    for (int i = 0; i < otherArgs.length - 1; ++i) {\n      FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n    }\n    FileOutputFormat.setOutputPath(job,\n      new Path(otherArgs[otherArgs.length - 1]));\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n}\n}"
	},
	"hcatalog": {
		"class": "HcatalogTrace",
		"code": "drop table if exists TracerHcatTest;\ncreate table TracerHcatTest (name string,id int) row format delimited fields terminated by ':' stored as textfile;\ndescribe TracerHcatTest;\ndrop table TracerHcatTest;"
	},
	"hivejdbc": {
		"class": "HiveJdbcTrace",
		"code": "import java.io.PrintWriter;\nimport java.sql.SQLException;\nimport java.sql.Connection;\nimport java.sql.ResultSet;\nimport java.sql.Statement;\nimport java.sql.DriverManager;\n\npublic class HiveJdbcClient {\n  private static String driverName = \"org.apache.hive.jdbc.HiveDriver\";\n\n  /**\n   * @param args\n   * @throws SQLException\n   */\n  public static void main(String[] args) throws SQLException {\n    try {\n      Class.forName(driverName);\n    } catch (ClassNotFoundException e) {\n      // TODO Auto-generated catch block\n      e.printStackTrace();\n      System.exit(1);\n    }\n\n    // set logging\n    DriverManager.setLogWriter(new PrintWriter(System.out));\n\n    // set login timeout\n    DriverManager.setLoginTimeout(10);\n\n    // show all available drivers\n    java.util.Enumeration e = DriverManager.getDrivers();\n    while (e.hasMoreElements()) {\n        Object driverAsObject = e.nextElement();\n        System.out.println(\"DRIVER = \" + driverAsObject);\n    }\n\n    System.out.println(\"Creating connection with drivermanager ...\");\n    Connection con = DriverManager.getConnection(%s);\n\n    System.out.println(\"Creating statement object from connection ...\");\n    Statement stmt = con.createStatement();\n\n    System.out.println(\"Calling show databases ...\");\n    stmt.execute(\"show databases\");\n\n    System.out.println(\"Calling show tables ...\");\n    stmt.execute(\"show tables\");\n\n    System.out.println(\"Calling create table ...\");\n    stmt.execute(\"create table hadooptracertest(a INT)\");\n\n    System.out.println(\"Calling drop table ...\");\n    stmt.execute(\"drop table hadooptracertest\");\n  }\n}",
		"data": "#!/bin/bash\n\n# write out the java code\ncat > HiveJdbcClient.java << EOF\n$CODE\nEOF\n\n# export the CLASSPATH\n$CLASSPATH\n\n# cleanup old binaries\nrm -f HiveJdbcClient.class\n\n# compile the java code\n$JDK HiveJdbcClient.java\n\n# find the timeout command\nTIMEOUT=$$(command -v timeout)\n\n# run the code with -verbose:class\nBASECMD=\"$JRE -verbose:class HiveJdbcClient\"\nrm -rf hivejava.debug\nif [ $$TIMEOUT != '' ]; then\n    echo \"Running with timeout\"\n    $$TIMEOUT -s SIGKILL 360s $$BASECMD 2>&1 | tee -a hivejava.debug\nelse\n    echo \"Running hive without timeout\"\n    $$BASECMD 2>&1 | tee -a hivejava.debug\nfi\n\n# check the exit code\nRC=$$?\nif [ $$RC != 0 ]; then\n    exit $$RC\nfi\n\n# check for any errors\negrep -e ^Error -e ^\"java.lang.ClassNotFoundException\" hivejava.debug\nRC=$$?\nif [ $$RC == 0 ]; then\n    exit 1\nfi"
	},
	"hcatapi": {
		"class": "HcatAPITrace",
		"code": "package org.hts.hcat;\nimport java.math.BigInteger;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Random;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hive.conf.HiveConf;\nimport org.apache.hadoop.hive.metastore.HiveMetaStore;\nimport org.apache.hadoop.hive.metastore.api.PartitionEventType;\nimport org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;\nimport org.apache.hadoop.hive.ql.io.RCFileInputFormat;\nimport org.apache.hadoop.hive.ql.io.RCFileOutputFormat;\nimport org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\nimport org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\nimport org.apache.hadoop.hive.ql.io.orc.OrcSerde;\nimport org.apache.hadoop.hive.ql.metadata.Table;\nimport org.apache.hadoop.hive.serde.serdeConstants;\nimport org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe;\nimport org.apache.hadoop.mapred.TextInputFormat;\nimport org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer;\nimport org.apache.hive.hcatalog.common.HCatConstants;\nimport org.apache.hive.hcatalog.common.HCatException;\nimport org.apache.hive.hcatalog.data.schema.HCatFieldSchema;\nimport org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Type;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.hadoop.util.Shell;\nimport org.apache.hive.hcatalog.api.HCatClient;\nimport org.apache.hive.hcatalog.api.HCatTable;\nimport org.apache.hive.hcatalog.api.HCatCreateTableDesc;\npublic class TestHCatClient {\n  private static final Logger LOG = LoggerFactory.getLogger(TestHCatClient.class);\n  private static final String msPort = \"9083\";\n  private static HiveConf hcatConf;\n  private static boolean isReplicationTargetHCatRunning = false;\n  private static final String replicationTargetHCatPort = \"9083\";\n  private static HiveConf replicationTargetHCatConf;\n  private static SecurityManager securityManager;\npublic static void main(String[] args) throws Exception {\n    hcatConf = new HiveConf(TestHCatClient.class);\n    hcatConf.setVar(HiveConf.ConfVars.METASTOREURIS, \"$thrift_uri\");\n    hcatConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTCONNECTIONRETRIES, 3);\n    hcatConf.setIntVar(HiveConf.ConfVars.METASTORETHRIFTFAILURERETRIES, 3);\n    System.out.println(hcatConf);\n    Boolean connected = false;\n    HCatClient client = null;\n    try {\n        client = HCatClient.create(new Configuration(hcatConf));\n        connected = true;\n    } catch (Exception e) {\n        e.printStackTrace();\n        connected = false;\n    }\n    if ( connected ) {\n        List<String> dbNames = client.listDatabaseNamesByPattern(\"*\");\n        System.out.println(dbNames);\n        String dbName = \"default\";\n        List<String> tbNames = client.listTableNamesByPattern(dbName, \"*\");\n        System.out.println(tbNames);\n        client.close();\n        System.exit(0);\n    } else {\n        System.exit(1);\n    }\n  }\n}"
	},
	"hbasehcat": {
		"class": "HcatalogTrace",
		"code": "drop table if exists hbase_table_1;\nCREATE TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf1:val\") TBLPROPERTIES (\"hbase.table.name\" = \"xyz\", \"hbase.mapred.output.outputtable\" = \"xyz\");\ndrop table hbase_table_1;"
	},
	"spark": {
		"class": "SparkTrace",
                "code": "package org.apache.spark.examples.sql;\nimport java.util.ArrayList;\nimport java.io.File;\nimport java.util.List;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.io.Serializable;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.function.Function;\nimport org.apache.spark.api.java.function.MapFunction;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.Encoder;\nimport org.apache.spark.sql.Encoders;\nimport org.apache.spark.sql.RowFactory;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.spark.sql.types.StructField;\nimport org.apache.spark.sql.types.StructType;\nimport org.apache.spark.sql.AnalysisException;\nimport static org.apache.spark.sql.functions.col;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.mllib.fpm.AssociationRules;\nimport org.apache.spark.mllib.fpm.FPGrowth;\nimport org.apache.spark.mllib.fpm.FPGrowth.FreqItemset;\nimport org.apache.spark.api.java.function.FlatMapFunction;\nimport org.apache.spark.sql.*;\nimport org.apache.spark.sql.streaming.StreamingQuery;\nimport java.util.Arrays;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.SparkConf;\npublic class JavaSparkSQLExample {\n  public static class Person implements Serializable {\n    private String name;\n    private int age;\n    public String getName() {\n      return name;\n    }\n    public void setName(String name) {\n      this.name = name;\n    }\n    public int getAge() {\n      return age;\n    }\n    public void setAge(int age) {\n      this.age = age;\n    }\n  }\n  public static class Record implements Serializable {\n    private int key;\n    private String value;\n    public int getKey() {\n      return key;\n    }\n    public void setKey(int key) {\n      this.key = key;\n    }\n    public String getValue() {\n      return value;\n    }\n    public void setValue(String value) {\n      this.value = value;\n    }\n  }\n  public static void main(String[] args) throws AnalysisException,Exception {\n    String warehouseLocation = new File(\"spark-warehouse\").getAbsolutePath();\n    SparkSession spark = SparkSession.builder().appName(\"Java Spark Hive Example\").config(\"spark.sql.warehouse.dir\", warehouseLocation).enableHiveSupport().getOrCreate();\n    spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\");\n    spark.sql(\"LOAD DATA LOCAL INPATH '\" + args[2] + \"' INTO TABLE src\");\n    spark.sql(\"SELECT * FROM src\").show();\n    spark.sql(\"SELECT COUNT(*) FROM src\").show();\n    Dataset<Row> sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\");\n    Dataset<String> stringsDS = sqlDF.map(new MapFunction<Row, String>() {\n      @Override\n      public String call(Row row) throws Exception {\n        return \"Key: \" + row.get(0) + \", Value: \" + row.get(1);\n      }\n    }, Encoders.STRING());\n    stringsDS.show();\n    List<Record> records = new ArrayList<>();\n    for (int key = 1; key < 100; key++) {\n      Record record = new Record();\n      record.setKey(key);\n      record.setValue(\"val_\" + key);\n      records.add(record);\n    }\n    Dataset<Row> recordsDF = spark.createDataFrame(records, Record.class);\n    recordsDF.createOrReplaceTempView(\"records\");\n    spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show();\n    spark.stop();\n\n    SparkConf sparkConf = new SparkConf().setAppName(\"JavaSparkSQLExample\");\n    JavaSparkContext sc = new JavaSparkContext(sparkConf);\n    JavaRDD<FPGrowth.FreqItemset<String>> freqItemsets = sc.parallelize(Arrays.asList(\n      new FreqItemset<>(new String[] {\"a\"}, 15L),\n      new FreqItemset<>(new String[] {\"b\"}, 35L),\n      new FreqItemset<>(new String[] {\"a\", \"b\"}, 12L)\n    ));\n    AssociationRules arules = new AssociationRules()\n      .setMinConfidence(0.8);\n    JavaRDD<AssociationRules.Rule<String>> results = arules.run(freqItemsets);\n    for (AssociationRules.Rule<String> rule : results.collect()) {\n      System.out.println(\n        rule.javaAntecedent() + \" => \" + rule.javaConsequent() + \", \" + rule.confidence());\n    }\n     sc.stop();\n    spark = SparkSession.builder().appName(\"Java Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate();\n    System.out.println(args[0]+args[1]);\n    runBasicDataFrameExample(spark,args[0]);\n    runDatasetCreationExample(spark,args[0]);\n    runInferSchemaExample(spark,args[1]);\n    runProgrammaticSchemaExample(spark,args[1]);\n    spark.stop();\n  }\n  private static void runBasicDataFrameExample(SparkSession spark,String FileName) throws AnalysisException {\n    Dataset<Row> df = spark.read().json(FileName);\n    df.show();\n    df.printSchema();\n    df.select(\"name\").show();\n    df.select(col(\"name\"), col(\"age\").plus(1)).show();\n    df.filter(col(\"age\").gt(21)).show();\n    df.groupBy(\"age\").count().show();\n    df.createOrReplaceTempView(\"people\");\n    Dataset<Row> sqlDF = spark.sql(\"SELECT * FROM people\");\n    sqlDF.show();\n  }\n  private static void runDatasetCreationExample(SparkSession spark,String FileName) {\n    Person person = new Person();\n    person.setName(\"Andy\");\n    person.setAge(32);\n    Encoder<Person> personEncoder = Encoders.bean(Person.class);\n    Dataset<Person> javaBeanDS = spark.createDataset(\n      Collections.singletonList(person),\n      personEncoder\n    );\n    javaBeanDS.show();\n    Encoder<Integer> integerEncoder = Encoders.INT();\n    Dataset<Integer> primitiveDS = spark.createDataset(Arrays.asList(1, 2, 3), integerEncoder);\n    Dataset<Integer> transformedDS = primitiveDS.map(new MapFunction<Integer, Integer>() {\n      @Override\n      public Integer call(Integer value) throws Exception {\n        return value + 1;\n      }\n    }, integerEncoder);\n    transformedDS.collect(); // Returns [2, 3, 4]\n    String path = FileName;\n    Dataset<Person> peopleDS = spark.read().json(path).as(personEncoder);\n    peopleDS.show();\n  }\n\n  private static void runInferSchemaExample(SparkSession spark,String FileName) {\n    JavaRDD<Person> peopleRDD = spark.read().textFile(FileName).javaRDD().map(new Function<String, Person>() {\n        @Override\n        public Person call(String line) throws Exception {\n          String[] parts = line.split(\",\");\n          Person person = new Person();\n          person.setName(parts[0]);\n          person.setAge(Integer.parseInt(parts[1].trim()));\n          return person;\n        }\n      });\n    Dataset<Row> peopleDF = spark.createDataFrame(peopleRDD, Person.class);\n    peopleDF.createOrReplaceTempView(\"people\");\n    Dataset<Row> teenagersDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\");\n    Encoder<String> stringEncoder = Encoders.STRING();\n    Dataset<String> teenagerNamesByIndexDF = teenagersDF.map(new MapFunction<Row, String>() {\n      @Override\n      public String call(Row row) throws Exception {\n        return \"Name: \" + row.getString(0);\n      }\n    }, stringEncoder);\n    teenagerNamesByIndexDF.show();\n    Dataset<String> teenagerNamesByFieldDF = teenagersDF.map(new MapFunction<Row, String>() {\n      @Override\n      public String call(Row row) throws Exception {\n        return \"Name: \" + row.<String>getAs(\"name\");\n      }\n    }, stringEncoder);\n    teenagerNamesByFieldDF.show();\n  }\n  private static void runProgrammaticSchemaExample(SparkSession spark,String FileName) {\n    JavaRDD<String> peopleRDD = spark.sparkContext().textFile(FileName, 1).toJavaRDD();\n    String schemaString = \"name age\";\n    List<StructField> fields = new ArrayList<>();\n    for (String fieldName : schemaString.split(\" \")) {\n      StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);\n      fields.add(field);\n    }\n    StructType schema = DataTypes.createStructType(fields);\n    JavaRDD<Row> rowRDD = peopleRDD.map(new Function<String, Row>() {\n      @Override\n      public Row call(String record) throws Exception {\n        String[] attributes = record.split(\",\");\n        return RowFactory.create(attributes[0], attributes[1].trim());\n      }\n    });\n    Dataset<Row> peopleDataFrame = spark.createDataFrame(rowRDD, schema);\n    peopleDataFrame.createOrReplaceTempView(\"people\");\n    Dataset<Row> results = spark.sql(\"SELECT name FROM people\");\n    Dataset<String> namesDS = results.map(new MapFunction<Row, String>() {\n      @Override\n      public String call(Row row) throws Exception {\n        return \"Name: \" + row.getString(0);\n      }\n    }, Encoders.STRING());\n    namesDS.show();\n  }\n}",
		"data": "238 val_238\n86 val_86\n311 val_311\n27 val_27\n165 val_165\n409 val_409\n255 val_255\n278 val_278\n98 val_98\n484 val_484\n265 val_265\n193 val_193\n401 val_401\n150 val_150\n273 val_273\n224 val_224\n369 val_369\n66 val_66\n128 val_128\n213 val_213\n146 val_146\n406 val_406\n429 val_429"
	},
	"hivejson": {
		"class": "HcatalogTrace",
		"code": "drop table if exists comments1;\nCREATE  EXTERNAL  TABLE comments1 (value STRING ) LOCATION  '/tmp/json';\ndrop table comments1;"
	},
        "ExtraJarFiles": ["jline-*","jruby-complete-*","hive-warehouse-connector-assembly-*"],
        "RemoveXMLProps": ["hive.execution.engine,hive-site.xml", "hadoop.security.group.mapping.provider.ad4users.ldap.bind.password,core-site.xml", "ssl.server.keystore.password,ssl-server.xml"]

}
